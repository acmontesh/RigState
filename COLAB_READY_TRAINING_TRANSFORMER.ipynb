{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQutPRM1G8ql"
   },
   "source": [
    "# Training Time Series Classifiers Based on Deep Learning\n",
    "<strong>Abraham C. Montes</strong> <br>\n",
    "<a href=\"https://www.linkedin.com/in/abraham-c-montes-6661a841/\">LinkedIn</a>|<a href=\"https://www.abraham-montes.com/\">Personal Site</a><br>\n",
    "The University of Texas at Austin | <a href=\"https://drilling.utexas.edu/\">RAPID research consortium</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPOnIFzdG_LB"
   },
   "source": [
    "<strong> Description: </strong> <br>\n",
    "The objective of this notebook is to train time-series classifiers based on deep learning for rig/well activity identification. The models implemented here are of two types: An LSTM recurrent neural network and a transformer for time series classification. Both models are constructed using the PyTorch library and leverage GPU hardware for faster, more efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "ak1Do99HOZKR"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "class LoggerDev:\n",
    "\n",
    "    def __init__( self,verbosity=2 ):\n",
    "      for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "      logging.basicConfig(   format='[%(levelname)s]......... %(message)s', level=logging.DEBUG   )\n",
    "      self.verbosity      =   verbosity\n",
    "\n",
    "    def setVerbosity( self,verbosity ):\n",
    "        self.verbosity      = verbosity\n",
    "        logging.info( f\"Verbosity has been set to {verbosity}\" )\n",
    "\n",
    "    def dbgMsg( self,msg=None ):\n",
    "        if (msg is not None) and (self.verbosity>1):\n",
    "            logging.debug(  msg  )\n",
    "\n",
    "    def infoMsg( self,msg=None ):\n",
    "        if (msg is not None) and (self.verbosity>1):\n",
    "            logging.info(  msg  )\n",
    "\n",
    "    def warningMsg( self,msg=None ):\n",
    "        if (msg is not None) and (self.verbosity>0):\n",
    "            logging.warning(  msg  )\n",
    "\n",
    "    def errorMsg( self,msg=None ):\n",
    "        if (msg is not None) and (self.verbosity>-1):\n",
    "            logging.error(  msg  )\n",
    "\n",
    "    def resultMessage( self,msg=None ):\n",
    "        if (msg is not None) and (self.verbosity>1):\n",
    "            logging.info(  f\"[RESULTS] -------->  {msg}\"  )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vgtswEHYOLjg"
   },
   "outputs": [],
   "source": [
    "class Nomenclature:\n",
    "\n",
    "    DATE_MNEMONIC           = \"Date\"\n",
    "    BIT_DEPTH_MNEMO         = \"Bit Depth [ft]\"\n",
    "    DEPTH_MNEMONIC          = \"Measured Depth [ft]\"\n",
    "    DEPTH_MNEMONIC_METRIC   = \"Measured Depth [m]\"\n",
    "    TORTUOSITY_MNEMO        = \"Tortuosity Idx\"\n",
    "    INCLINATION_MNEMO       = 'Inclination [°]'\n",
    "    AZIMUTH_MNEMO           = 'Azimuth [°]'\n",
    "    DLS_MNEMO               = 'DLS [°/100 ft]'\n",
    "    GAMMA_RAY_LOG_MNEMO     = \"Gamma Ray [API]\"\n",
    "    ROP_MNEMO               = \"ROP [fph]\"\n",
    "    TORQUE_MNEMO            = \"Torque [lb-ft]\"\n",
    "    WOB_MNEMO               = \"Weight on Bit [klb]\"\n",
    "    STANDPIPE_PRESSURE_MNEMO=\"Standpipe Pressure [psi]\"\n",
    "    RPM_MNEMO               = \"Surface Rotation [rpm]\"\n",
    "    HOOK_LOAD_MNEMO         = \"Hook Load [klb]\"\n",
    "    HOLE_DEPTH_MNEMO        = \"Hole Depth [ft]\"\n",
    "    FLOW_IN_MNEMO           = \"Flow In [gpm]\"\n",
    "    BLOCK_POSITION_MNEMO    = \"Block Position [ft]\"\n",
    "    RIG_STATE_MNEMO         = \"Rig State\"\n",
    "    RIG_ACTIVITY_MNEMO      = \"Activity\"\n",
    "    SECTION_PHASE_MNEMO     = \"Section Operation Phase\"\n",
    "    TORTUOSITY_BIT_MNEMO    = 'Tortuosity Idx at Bit'\n",
    "    FLEX_RIGIDITY_BIT_MNEMO = 'Flex Rigidity Difference at Bit'\n",
    "    BLOCK_WEIGHT_MNEMO      = 'Block Weight [klb]'\n",
    "    TRIP_OUT_MNEMO          = \"Trip Out\"\n",
    "    DRILLING_MNEMO          = \"Drilling\"\n",
    "    CIRCULATION_MNEMO       = \"Circulation\"\n",
    "    BLOCK_WEIGHT_DELTA_MNEMO= \"delta\"\n",
    "    BLOCK_WEIGHT_DELTA_DATES_MNEMO=\"dates\"\n",
    "    HOOK_LOAD_MEAN_MNEMO    = 'Effective Hook Load Mean'\n",
    "    PRESSURE_MEAN_MNEMO     = 'Standpipe Pressure Mean'\n",
    "    BLOCK_POSITION_TREND_MNEMO='Block Position Trend'\n",
    "    FLOW_RATE_VARIABILITY_MNEMO='Flow Rate Variability'\n",
    "    FLOW_RATE_MEAN_MNEMO        ='Flow Rate Mean'\n",
    "    RPM_MEAN_MNEMO          = 'RPM Mean'\n",
    "    HOOK_LOAD_MEAN_MNEMO    = 'Hook Load Mean'\n",
    "    HOOK_LOAD_VARIABILITY_MNEMO='Hook Load Variability'\n",
    "    EFF_HOOK_LOAD_MNEMO     = 'Effective Hook Load [klb]'\n",
    "    ROP_MEAN_MNEMO          = 'ROP Mean'\n",
    "    BLOCK_POSITION_TREND_SHORT_MNEMO='Short-Term Block Position Trend'\n",
    "    BLOCK_POSITION_TREND_LONG_MNEMO='Long-Term Block Position Trend'\n",
    "    HOOK_LOAD_SHORT_TREND='Short-Term Effective Hook Load Trend'\n",
    "    HOOK_LOAD_LONG_TREND='Long-Term Effective Hook Load Trend'\n",
    "    BACKREAMING_MNEMO       = 117\n",
    "    TRIP_OUT_ELEV_MNEMO     = 112\n",
    "    PUMPING_OUT_MNEMO       = 115\n",
    "    REAMING_MNEMO           = 116\n",
    "    DRILLING_ROT_MNEMO      = 119\n",
    "    CONNECTION_MNEMO        = 118\n",
    "    TRANSFORMER_MODEL_MNEMO = \"transformer\"\n",
    "    LSTM_MODEL_MNEMO        = \"lstm\"\n",
    "    GOAL_RIG_STATES         = [111,112,114,115,116,117,118,119,120,121,124]\n",
    "    DICT_RIG_STATES         = {\n",
    "                                    111:\"Tripping in on elevators\",\n",
    "                                    112:\"Tripping out on elevators\",\n",
    "                                    114:\"Washing down\",\n",
    "                                    115:\"Pumping out\",\n",
    "                                    116:\"Reaming\",\n",
    "                                    117:\"Backreaming\",\n",
    "                                    118:\"Drillstring set in the slips\",\n",
    "                                    119:\"Drilling (with surface rotation)\",\n",
    "                                    120:\"Drilling (sliding)\",\n",
    "                                    121:\"Circulating\",\n",
    "                                    124:\"Static, off-the-slips drillstring\"\n",
    "                                }\n",
    "    CONVERTION_HASH_TABLE   = {\n",
    "            \"TIME\":                             DATE_MNEMONIC,\n",
    "            \"DBTM\":                             BIT_DEPTH_MNEMO,\n",
    "            \"BPOS\":                             BLOCK_POSITION_MNEMO,\n",
    "            \"MFIA\":                             FLOW_IN_MNEMO,\n",
    "            \"DMEA\":                             HOLE_DEPTH_MNEMO,\n",
    "            \"HKLA\":                             HOOK_LOAD_MNEMO,\n",
    "            \"RPMA\":                             RPM_MNEMO,\n",
    "            \"SPPA\":                             STANDPIPE_PRESSURE_MNEMO,\n",
    "            \"WOBA\":                             WOB_MNEMO,\n",
    "            \"TQA\":                              TORQUE_MNEMO,\n",
    "            # \"ROPA__\":                           ROP_MNEMO,\n",
    "            #Note (10/21/24): For some unknown issue within Shell's system, the 'ROPA__' channel\n",
    "            # is sometimes unavailable (full with NaNs). An alternative is the one below.\n",
    "            \"ROPA__SHELL_CALCULATION_INPUT_TIME\":ROP_MNEMO,\n",
    "            \"MD\":                               DEPTH_MNEMONIC,\n",
    "            \"INCLINATION\":                      INCLINATION_MNEMO,\n",
    "            \"AZIMUTH\":                          AZIMUTH_MNEMO,\n",
    "            \"DLS\":                              DLS_MNEMO,\n",
    "            \"DEPTH\":                            DEPTH_MNEMONIC,\n",
    "            \"SGRC\":                             GAMMA_RAY_LOG_MNEMO\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "owhipDAbOS4m"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class TimeSeriesTransformer( nn.Module ):\n",
    "\n",
    "    def __init__( self, nInputs,dModel,nHead,nLayers,dimFeedForward,nOutput,dropoutRate=0.1 ):\n",
    "        super( TimeSeriesTransformer,self ).__init__( )\n",
    "        self.D                  = nInputs\n",
    "        self.dModel             = dModel\n",
    "        self.inputProjection    = nn.Linear(  nInputs, dModel  )\n",
    "        self.posEncoding        = nn.Parameter( self._generatePosEncoding( dModel, maxLength=500 ), requires_grad=False )\n",
    "        encoderLayer            = nn.TransformerEncoderLayer( d_model=dModel, nhead=nHead, dim_feedforward=dimFeedForward, dropout=dropoutRate )\n",
    "        self.transformerEncoder = nn.TransformerEncoder(encoderLayer, num_layers=nLayers)\n",
    "        self.fc                 = nn.Linear( dModel, nOutput  )\n",
    "        self.softmax            = nn.Softmax( dim=1 )\n",
    "\n",
    "    def _generatePosEncoding(  self, dModel, maxLength  ):\n",
    "        position                = torch.arange(  0, maxLength  ).unsqueeze(1)\n",
    "        divTerm                 = torch.exp(torch.arange(  0, dModel, 2) * -(np.log(10000.0) / dModel)  )\n",
    "        posEncoding             = torch.zeros(  maxLength, dModel  )\n",
    "        posEncoding[ :, 0::2 ]    = torch.sin(  position * divTerm  )\n",
    "        posEncoding[ :, 1::2 ]    = torch.cos(  position * divTerm  )\n",
    "        return posEncoding.unsqueeze( 0 )\n",
    "\n",
    "    def forward( self, x ):\n",
    "        x                       = self.inputProjection( x )\n",
    "        x                       = x + self.posEncoding[ :, :x.size(1), : ]\n",
    "        x                       = x.permute(1, 0, 2)\n",
    "        x                       = self.transformerEncoder( x )\n",
    "        x                       = x[ -1, :, : ]\n",
    "        output                  = self.fc( x )\n",
    "        return self.softmax( output )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "9RV9k_v6HM3L"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)\n",
    "import scipy as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sb\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import joblib\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    FDICT_PLOTS                 = {'family':'Arial','size':8}\n",
    "    LOWER_LIM_HKL_FOR_BLOCK_WEIGHT      = 100\n",
    "    UPPER_LIM_HKL_FOR_BLOCK_WEIGHT      = 1000\n",
    "    WINDOW_BLOCK_POS_TREND              = 3\n",
    "\n",
    "    def __init__( self ):\n",
    "        \"\"\"\n",
    "        slidingWindow:          Time windows to derive features. For example, the trend of the block position. \n",
    "                                It is a list. The first component is a small time window, to emphasize short-term changes. \n",
    "                                The second one is a large time window, to account for long-term variations.\n",
    "        slidingWindowCoverage:  Time window of size T used to convert the extracted features from a NxD matrix to a NxTxD, where T is the size of the subsequences\n",
    "                                that the model will use as inputs.\n",
    "        \"\"\"\n",
    "        self.logger             = LoggerDev(  )\n",
    "        self.dataSets           = [  ]\n",
    "        self.concatDataset      = [  ]\n",
    "        self.blockWeights       = [  ]\n",
    "        self.nom                = Nomenclature(  )\n",
    "        self.labelPreds         = {  }\n",
    "        self.scaler             = StandardScaler( )\n",
    "        self.slidingWindow= None\n",
    "        self.slidingWindowCoverage= None\n",
    "\n",
    "    def setSlidingWindow( self,slidingWindows ):\n",
    "        self.slidingWindow      = slidingWindows\n",
    "\n",
    "    def setSlidingWindowCoverage( self,slidingWindowCoverage ):\n",
    "        self.slidingWindowCoverage      = slidingWindowCoverage\n",
    "\n",
    "    def loadData( self,pathFolder,blockWeights={  },nBinsBWInference=100,dropNaNs=True ):\n",
    "        for file in os.listdir( pathFolder ):\n",
    "            if file==\".ipynb_checkpoints\":  continue\n",
    "            df                                  = pd.read_csv( pathFolder+\"/\"+file,parse_dates=[ self.nom.DATE_MNEMONIC ],na_values=[-999.25] )\n",
    "            if dropNaNs:                        df                                  = df.dropna( axis=0 )\n",
    "            if file[ :-4 ] in list( blockWeights.keys(  ) ):\n",
    "                bW                              =   blockWeights[ file[ :-4 ] ]\n",
    "            else:\n",
    "                bW                              =   self._getBlockWeight( df,nbins=nBinsBWInference )\n",
    "                self.logger.warningMsg( f\"The block weight for the {file[ :-4 ]} well was not provided. Therefore, it has been inferred. Its value is: {bW:.1f}\" )\n",
    "            df[ self.nom.BLOCK_WEIGHT_MNEMO ]   = bW\n",
    "            self.dataSets.append( df )\n",
    "            self.blockWeights.append( bW )\n",
    "            self.logger.infoMsg( f\"Block weight for {file} has been set on: {bW:.1f} klb.\" )\n",
    "        self.logger.infoMsg( f\"Data has been loaded correctly to the trainer. In total, {len( self.dataSets )} dataframes have been loaded.\" )\n",
    "\n",
    "    def _loadCheckPoint( self, currentCheckPointPath, model, optimizer ):\n",
    "        checkpoint                              = torch.load(  currentCheckPointPath, weights_only=False  )\n",
    "        model.load_state_dict(  checkpoint['model_state_dict']  )\n",
    "        optimizer.load_state_dict(  checkpoint['optimizer_state_dict']  )\n",
    "        return model, optimizer\n",
    "\n",
    "    def trainModel(  self, modelType, batchSize=32, nEpochs=200, learningRate=0.0001, currentCheckPointPath=None,\n",
    "                    saveModel=True, savePath=\"model_transformer.pth\", saveScaler=True, scalerPath=\"scaler_transformer\",\n",
    "                     stratifyData=True,fitScaler=True, **kwargs  ):\n",
    "        if len( self.dataSets )==0:\n",
    "            self.logger.errorMsg( \"No data has been loaded to the trainer. Use the loadData( ) function before training a model.\" )\n",
    "            sys.exit( 1 );\n",
    "        model, optimizer        = self._createModel( modelType,learningRate, **kwargs )\n",
    "        self.logger.infoMsg( f\"The {modelType} model has been created.\" )\n",
    "        currEpoch               = 0\n",
    "        trainLosses             = np.zeros( nEpochs )\n",
    "        if 'slidingWindow' not in kwargs:\n",
    "            self.slidingWindow  = [5,30]\n",
    "            self.logger.warningMsg( f\"The size of the sliding windows (two resolution levels) has not been specified. By default, the small time window has been set on {self.slidingWindow[0]} sec, and the large one to {self.slidingWindow[1]} sec.\" )\n",
    "        else: self.slidingWindow= kwargs['slidingWindow']\n",
    "        if 'slidingWindowCoverage' not in kwargs:\n",
    "            self.slidingWindowCoverage  = 5\n",
    "            self.logger.warningMsg( f\"The size of the transformation window has not been specified. By default, it has been set on {self.slidingWindowCoverage}.\" )\n",
    "        else: self.slidingWindowCoverage= kwargs['slidingWindowCoverage']\n",
    "        if currentCheckPointPath is not None:\n",
    "            model, optimizer    = self._loadCheckPoint( currentCheckPointPath, model, optimizer )\n",
    "            self.logger.infoMsg( \"Checkpoint loaded successfully. Resuming training...\" )\n",
    "        else:                   self.logger.infoMsg( \"Starting training...\" )\n",
    "        device                  = torch.device( \"cuda:0\" if torch.cuda.is_available( ) else \"cpu\" )\n",
    "        typeDevice              = \"GPU\" if torch.cuda.is_available( ) else \"CPU\"\n",
    "        self.logger.infoMsg(  f\"Working on: {typeDevice}, model {torch.cuda.get_device_name(0)}\"  )\n",
    "        self.logger.infoMsg( \"Extracting features for dataset No. 1.\" )\n",
    "        X_train, y_train        = self.extractFeatures( self.dataSets[ 0 ],self.blockWeights[ 0 ],modelType=modelType,slidingWindow=self.slidingWindow,fitScaler=fitScaler )\n",
    "        if len( self.dataSets )>1:\n",
    "            for k,ds in enumerate( self.dataSets[1:] ):\n",
    "                self.logger.infoMsg( f\"Extracting features for dataset No. {k+2}.\" )\n",
    "                X_train_temp, y_train_temp  = self.extractFeatures( ds,self.blockWeights[ k+1 ],modelType=modelType,slidingWindow=self.slidingWindow,fitScaler=fitScaler )\n",
    "                X_train         = np.concatenate( [X_train,X_train_temp],axis=0 )\n",
    "                y_train         = np.concatenate( [y_train,y_train_temp] )\n",
    "        self.logger.infoMsg( f\"The training dataset has the following shape: {X_train.shape[0]} X {X_train.shape[1]}. The response: {y_train.shape}\" )\n",
    "        X_train                 = self.scaler.transform( X_train )\n",
    "        X_train, y_train        = self.wrapTensor( X_train,y_train,slidingWindow=self.slidingWindowCoverage )\n",
    "        nNans                   = np.isnan( X_train ).sum(  )\n",
    "        if nNans>0:             self.logger.warningMsg( f\"There are {nNans} NaNs in the training dataset. Consider trimming NaNs before training.\" )\n",
    "        X_train, y_train        = self._stratifyData( X_train, y_train,stratify=stratifyData )\n",
    "        X_train                 = torch.from_numpy( X_train.astype(np.float32) ).to( device )\n",
    "        y_train                 = torch.from_numpy( y_train.astype(np.float32) )\n",
    "        y_train                 = torch.argmax( y_train, dim=1 ).to( device )\n",
    "        self.logger.infoMsg( f\"Size of the training data: {(X_train.numel(  ) * X_train.element_size(  ))/1E9:.2f} GB for the input matrix and {(y_train.numel(  ) * y_train.element_size(  ))/1E9:.2f} GB for the response variable.\" )\n",
    "        if saveScaler:\n",
    "            joblib.dump( self.scaler, f\"{scalerPath}_CV{self.slidingWindowCoverage}.pkl\" )\n",
    "            self.logger.infoMsg( f\"Successfully saved scaler: {scalerPath}\" )\n",
    "        criterion               = nn.CrossEntropyLoss(  )\n",
    "        dataset                 = TensorDataset( X_train, y_train )\n",
    "        dataLoader              = DataLoader( dataset, batch_size=batchSize, shuffle=True )\n",
    "        self.logger.infoMsg( \"Data loaded to the torch DataLoader object.\" )\n",
    "        for epoch in range( nEpochs ):\n",
    "            if epoch==0: self.logger.infoMsg( f\"Initiating forward pass (epoch {epoch}).\" )\n",
    "            model.train(  )\n",
    "            runningLoss         = 0.0\n",
    "            correctPreds        = 0\n",
    "            totalPreds          = 0\n",
    "            for batchNo, ( inputs, labels ) in enumerate( dataLoader ):\n",
    "                if batchNo%150==0:self.logger.infoMsg( f'[LOADER>>>]..... Processing batch No. {batchNo}.')\n",
    "                optimizer.zero_grad(  )\n",
    "                outputs         = model( inputs )\n",
    "                loss            = criterion( outputs, labels )\n",
    "                loss.backward(  )\n",
    "                optimizer.step(  )\n",
    "                runningLoss     += loss.item(  )\n",
    "                _, predicted    = torch.max( outputs, 1 )\n",
    "                correctPreds    += ( predicted == labels ).sum(  ).item(  )\n",
    "                totalPreds      += labels.size( 0 )\n",
    "            epochLoss               = runningLoss / len(  dataLoader  )\n",
    "            trainLosses[ epoch ] = epochLoss\n",
    "            self.logger.infoMsg( f'[TRAINING MSG>>>]..... Epoch {epoch+1}/{nEpochs}, Train Loss: {loss.item(  ):.4f}')\n",
    "            self._saveCheckpoint( model,optimizer,epoch,trainLosses[-1],modelType,codeName=f\"_transformer_checkpoint.chpt\" )\n",
    "            self.logger.infoMsg( f\"Successfully saved checkpoint: {modelType}.chpt\" )\n",
    "        if saveModel:\n",
    "            torch.save(  model.state_dict(  ), savePath  )\n",
    "            self.logger.infoMsg( f\"Successfully saved {modelType} model: {savePath}\" )\n",
    "        return trainLosses, model\n",
    "    \n",
    "    def _stratifyData( self,X,y,stratify,nUndersamplingRounds=2 ):\n",
    "        if not stratify:    return X,y\n",
    "        yComp                   = np.argmax( y,axis=1 )\n",
    "        self.logger.infoMsg( f\"The number of unique classes are: {len(np.unique(yComp))}\" )\n",
    "        countDict                   = { x:np.sum( yComp==x ) for x in np.unique( yComp ) }\n",
    "        self.logger.infoMsg( f\"Before undersampling, the counts per class are: {countDict}.\" )\n",
    "        newX,newY               = ( X,y )\n",
    "        newX,newY,countDict     = self._undersampling( newX,newY,nTimes=nUndersamplingRounds )\n",
    "        self.logger.infoMsg( f\"After undersampling, the counts per class are: {countDict}.\" )\n",
    "        return newX,newY    \n",
    "    \n",
    "    def _undersampling( self,newX,newY,nTimes=2 ):\n",
    "        buffX,buffY             = ( newX,newY )\n",
    "        for i in range( nTimes ):\n",
    "            yComp                   = np.argmax( buffY,axis=1 )\n",
    "            countDict               = { x:np.sum( yComp==x ) for x in np.unique( yComp ) }\n",
    "            classRef                = sorted( countDict,key=countDict.get,reverse=True )[i]\n",
    "            classRefNext            = sorted( countDict,key=countDict.get,reverse=True )[i+1]\n",
    "            diff                    = countDict[classRef] - countDict[classRefNext]\n",
    "            rdSetDelete             = np.array( [ ] )\n",
    "            for jClass in sorted( countDict,key=countDict.get,reverse=True )[0:i+1]:\n",
    "                indices             = np.argwhere( yComp==jClass ).reshape( -1, )\n",
    "                selection           = np.random.choice( indices, size=diff, replace=False )\n",
    "                rdSetDelete         = np.concatenate( [ rdSetDelete,selection ] )\n",
    "            allIdxs                 = np.setdiff1d( np.arange( buffX.shape[0] ), rdSetDelete ).astype( int )\n",
    "            buffX                    = buffX[ allIdxs ]\n",
    "            buffY                    = buffY[ allIdxs ]\n",
    "        yComp                       = np.argmax( buffY,axis=1 )\n",
    "        countDict                   = { x:np.sum( yComp==x ) for x in np.unique( yComp ) }\n",
    "        return buffX,buffY,countDict\n",
    "\n",
    "    def wrapTensor( self,X,y,slidingWindow ):\n",
    "        XF                      = [ ]\n",
    "        yF                      = [ ]\n",
    "        for t in range( X.shape[ 0 ]-slidingWindow ):\n",
    "            x                   = X[ t:t+slidingWindow,: ]\n",
    "            XF.append( x )\n",
    "            ny                  = y[ t+slidingWindow-1,: ]\n",
    "            yF.append( ny )\n",
    "        XF                      = np.array( XF ).reshape( -1,slidingWindow,X.shape[1] )\n",
    "        yF                      = np.array( yF ).reshape( -1,y.shape[1] )\n",
    "        # if modelType==\"lstm\":\n",
    "        #     y_train                 = torch.from_numpy(yF.astype(np.float32)).to( device )\n",
    "        # else:\n",
    "        #     y_train                 = torch.from_numpy(yF.astype(np.float32))\n",
    "        #     y_train                 = torch.argmax( y_train, dim=1 ).to( device )\n",
    "        return XF.astype(np.float32),yF.astype(np.float32)\n",
    "\n",
    "    def _getBlockWeight( self,dataSet,nbins=100 ):\n",
    "        df                      = dataSet.copy( deep=True )\n",
    "        df[self.nom.HOOK_LOAD_MNEMO]   = np.where( (df[ self.nom.HOOK_LOAD_MNEMO ]<0) | (df[ self.nom.HOOK_LOAD_MNEMO ]>self.UPPER_LIM_HKL_FOR_BLOCK_WEIGHT), 0, df[ self.nom.HOOK_LOAD_MNEMO ])\n",
    "        b                       = np.flip( np.argsort( np.histogram( df[ self.nom.HOOK_LOAD_MNEMO ],bins=nbins )[0] ) )\n",
    "        c                       = np.histogram( df[ self.nom.HOOK_LOAD_MNEMO ],bins=nbins )[1][b][  np.histogram( df[ self.nom.HOOK_LOAD_MNEMO ],bins=nbins )[1][b]<self.LOWER_LIM_HKL_FOR_BLOCK_WEIGHT  ]\n",
    "        delta                   = ( np.histogram( df[ self.nom.HOOK_LOAD_MNEMO ],bins=nbins )[1].max(  ) - np.histogram( df[ self.nom.HOOK_LOAD_MNEMO ],bins=nbins )[1].min(  ) )/(  2*nbins  )\n",
    "        inferredBW              = c[0]+delta\n",
    "        return inferredBW\n",
    "\n",
    "    def extractFeatures( self, df, blockWeight, modelType,slidingWindow=[5,60],fitScaler=False ):\n",
    "        if isinstance( slidingWindow,list ):\n",
    "            if len( slidingWindow )>=2:\n",
    "                shortTW,longTW                              = slidingWindow\n",
    "            else: \n",
    "                self.logger.errorMsg( \"slidingWindow must be a list with at least two components: The size of the time windows for inspecting short- and long-term variations.\" )\n",
    "                sys.exit( 1 )\n",
    "        else:\n",
    "            self.logger.errorMsg( \"slidingWindow must be a list.\" )\n",
    "            sys.exit( 1 )\n",
    "        df[self.nom.EFF_HOOK_LOAD_MNEMO]            = np.where(df[self.nom.HOOK_LOAD_MNEMO]>0,df[self.nom.HOOK_LOAD_MNEMO]-df[self.nom.BLOCK_WEIGHT_MNEMO],0)\n",
    "        df[self.nom.BLOCK_POSITION_TREND_SHORT_MNEMO]= df[self.nom.BLOCK_POSITION_MNEMO].rolling(window=shortTW).apply(self._trend,raw=True,engine='cython')\n",
    "        df[self.nom.BLOCK_POSITION_TREND_LONG_MNEMO]= df[self.nom.BLOCK_POSITION_MNEMO].rolling(window=longTW).apply(self._trend,raw=True,engine='cython')\n",
    "        df[self.nom.FLOW_RATE_VARIABILITY_MNEMO]    = df[self.nom.FLOW_IN_MNEMO].rolling(window=shortTW).std(  )\n",
    "        df[self.nom.FLOW_RATE_MEAN_MNEMO]           = df[self.nom.FLOW_IN_MNEMO].rolling(window=shortTW).mean(  )\n",
    "        df[self.nom.PRESSURE_MEAN_MNEMO]            = df[self.nom.STANDPIPE_PRESSURE_MNEMO].rolling(window=shortTW).mean(  )\n",
    "        df[self.nom.RPM_MEAN_MNEMO]                 = df[self.nom.RPM_MNEMO].rolling(window=shortTW).mean(  )\n",
    "        df[self.nom.HOOK_LOAD_MEAN_MNEMO]           = df[self.nom.EFF_HOOK_LOAD_MNEMO].rolling(window=shortTW).mean(  )\n",
    "        df[self.nom.HOOK_LOAD_SHORT_TREND]                = df[self.nom.EFF_HOOK_LOAD_MNEMO].rolling(window=shortTW).apply(self._trend,raw=True,engine='cython')\n",
    "        df[self.nom.HOOK_LOAD_LONG_TREND]                = df[self.nom.EFF_HOOK_LOAD_MNEMO].rolling(window=longTW).apply(self._trend,raw=True,engine='cython')\n",
    "        # df[self.nom.HOOK_LOAD_VARIABILITY_MNEMO]    = df[self.nom.EFF_HOOK_LOAD_MNEMO].rolling(window=shortTW).std(  )\n",
    "        df[self.nom.ROP_MEAN_MNEMO]                 = df[self.nom.ROP_MNEMO].rolling(window=shortTW).mean(  )\n",
    "        dfTraining                                  = df[[self.nom.BLOCK_POSITION_TREND_SHORT_MNEMO,\n",
    "                                                          self.nom.BLOCK_POSITION_TREND_LONG_MNEMO,\n",
    "                                                            self.nom.FLOW_RATE_VARIABILITY_MNEMO,\n",
    "                                                            self.nom.FLOW_RATE_MEAN_MNEMO,\n",
    "                                                            self.nom.PRESSURE_MEAN_MNEMO,\n",
    "                                                            self.nom.RPM_MEAN_MNEMO,\n",
    "                                                            self.nom.HOOK_LOAD_MEAN_MNEMO,\n",
    "                                                            self.nom.ROP_MEAN_MNEMO,\n",
    "                                                            self.nom.HOOK_LOAD_SHORT_TREND,\n",
    "                                                            self.nom.HOOK_LOAD_LONG_TREND,\n",
    "                                                            self.nom.RIG_STATE_MNEMO]]\n",
    "        dfTraining                                  = dfTraining.dropna( axis=0 )\n",
    "        X                       = dfTraining.iloc[:,:-1].values\n",
    "        y                       = dfTraining[self.nom.RIG_STATE_MNEMO].values\n",
    "        y                       = pd.get_dummies( y )\n",
    "        y                       = y.reindex( columns=self.nom.GOAL_RIG_STATES,fill_value=False )\n",
    "        y                       = y.values        \n",
    "        self.scaler.partial_fit( X )\n",
    "        return X, y\n",
    "    \n",
    "\n",
    "\n",
    "    def plotLosses( self, trainingLoss, testLoss, outPath=\"loss_curve.png\" ):\n",
    "        fig,ax                    = plt.subplots( figsize=(6.83,3.33) )\n",
    "        ax.plot( np.arange( len(trainingLoss) ), trainingLoss, lw=1.5, color='teal',label=\"Training Loss\" )\n",
    "        if not testLoss is None: ax.plot( np.arange( len(testLoss) ), testLoss, lw=1.5, color='red',ls='--',label=\"Test Loss\" )\n",
    "        ax.set_xlabel( 'Epoch', fontdict={ **self.FDICT_PLOTS,'weight':'bold'} )\n",
    "        ax.set_ylabel( 'Loss (Cross Entropy)', fontdict={ **self.FDICT_PLOTS,'weight':'bold'} )\n",
    "        ax.set_xticklabels( ax.get_xticklabels( ), fontdict=self.FDICT_PLOTS )\n",
    "        ax.set_yticklabels( ax.get_yticklabels( ), fontdict=self.FDICT_PLOTS )\n",
    "        ax.yaxis.set_minor_locator( AutoMinorLocator( ) )\n",
    "        ax.xaxis.set_minor_locator( AutoMinorLocator( ) )\n",
    "        ax.legend(  )\n",
    "        plt.tight_layout(  )\n",
    "        fig.savefig( outPath,dpi=600 )\n",
    "\n",
    "    def _plotFancyContingencyTable(  self, confMatrix, classNames,outPath=\"contingency_table.png\"  ):\n",
    "        fig, ax                 = plt.subplots(  figsize=(  6.83, 4  ))\n",
    "        buff                    = sb.heatmap(  confMatrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                        xticklabels=classNames, yticklabels=classNames, ax=ax, annot_kws={ **self.FDICT_PLOTS }  )\n",
    "        ax.set_xlabel(  \"Predicted Labels\",fontdict={ **self.FDICT_PLOTS, 'weight':'bold' }  )\n",
    "        ax.set_ylabel(  \"True Labels\",fontdict={ **self.FDICT_PLOTS, 'weight':'bold' }  )\n",
    "        ax.set_title(  \"Confusion Matrix\",fontdict={ **self.FDICT_PLOTS, 'weight':'bold' }  )\n",
    "        ax.set_xticklabels( ax.get_xticklabels( ), fontdict=self.FDICT_PLOTS )\n",
    "        ax.set_yticklabels( ax.get_yticklabels( ), fontdict=self.FDICT_PLOTS )\n",
    "        plt.tight_layout(  )\n",
    "        fig.savefig( outPath,dpi=600 )\n",
    "\n",
    "    def _saveCheckpoint( self,model, optimizer, epoch, loss, modelType, path='checkpoint.pth',codeName=\"_\" ):\n",
    "        newPath    = f\"{modelType}{codeName}_CV{self.slidingWindowCoverage}\"\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(  ),\n",
    "            'optimizer_state_dict': optimizer.state_dict(  ),\n",
    "            'loss': loss\n",
    "        }\n",
    "        torch.save(  checkpoint, newPath  )\n",
    "        self.logger.infoMsg( f\"Checkpoint saved at epoch {epoch}.\" )\n",
    "\n",
    "    def _createModel( self,modelType,learningRate=0.0001,**kwargs ):\n",
    "        device = torch.device( \"cuda:0\" if torch.cuda.is_available( ) else \"cpu\" )\n",
    "        if modelType==self.nom.TRANSFORMER_MODEL_MNEMO:\n",
    "            model                   = self.createModelTransformer( device, **kwargs )\n",
    "        elif modelType==self.nom.LSTM_MODEL_MNEMO:\n",
    "            model                   = self.createModelLSTM( device, **kwargs )\n",
    "        optimizer                   = torch.optim.Adam( model.parameters( ),lr=learningRate )\n",
    "        return model,optimizer\n",
    "\n",
    "    def createModelTransformer( self, device, **kwargs ):\n",
    "        nInputs         = 10 if \"nInputs\" not in kwargs else kwargs[\"nInputs\"]\n",
    "        nLayers         = 3 if \"nLayers\" not in kwargs else kwargs[\"nLayers\"]\n",
    "        dModel          = 256 if \"dModel\" not in kwargs else kwargs[\"dModel\"]\n",
    "        nOutput         = 11 if \"nOutput\" not in kwargs else kwargs[\"nOutput\"]\n",
    "        nHead           = 8 if \"nHead\" not in kwargs else kwargs[\"nHead\"]\n",
    "        dimFeedForward  = 512 if \"dimFeedForward\" not in kwargs else kwargs[\"dimFeedForward\"]\n",
    "        model           = TimeSeriesTransformer(  nInputs=nInputs,nLayers=nLayers,dModel=dModel,\n",
    "                                        nOutput=nOutput,nHead=nHead,dimFeedForward=dimFeedForward )\n",
    "        model.to( device )\n",
    "        return model\n",
    "\n",
    "    def createModelLSTM( self,device, **kwargs ):\n",
    "        nInputs         = 7 if \"nInputs\" not in kwargs else kwargs[\"nInputs\"]\n",
    "        nLayers         = 3 if \"nLayers\" not in kwargs else kwargs[\"nLayers\"]\n",
    "        nHidden         = 30 if \"nHidden\" not in kwargs else kwargs[\"nHidden\"]\n",
    "        nOutput         = 10 if \"nOutput\" not in kwargs else kwargs[\"nOutput\"]\n",
    "        model           = LSTMClassifier(  nInputs=nInputs,nLayers=nLayers, nOutput=nOutput,nHidden=nHidden )\n",
    "        model.to( device )\n",
    "        return model\n",
    "\n",
    "    def _trend( self,window ):\n",
    "        slope,_,_,_,_         = sp.stats.linregress( np.arange(len( window )), window )\n",
    "        return slope\n",
    "\n",
    "    def testModel( self,pathTest,pathScaler=None,blockWeights={  },batchSize=32,loadFromPath=None,modelType=None,model=None,fitScaler=False,**kwargs ):\n",
    "        device                  = torch.device( \"cuda:0\" if torch.cuda.is_available( ) else \"cpu\" )\n",
    "        typeDevice              = \"GPU\" if torch.cuda.is_available( ) else \"CPU\"\n",
    "        self.logger.infoMsg(  f\"Testing on: {typeDevice}, model {torch.cuda.get_device_name(0)}\"  )\n",
    "        if 'slidingWindow' not in kwargs:\n",
    "            self.slidingWindow  = [5,30]\n",
    "            self.logger.warningMsg( f\"The size of the sliding windows (two resolution levels) has not been specified. By default, the small time window has been set on {self.slidingWindow[0]} sec, and the large one to {self.slidingWindow[1]} sec.\" )\n",
    "        else: self.slidingWindow= kwargs['slidingWindow']\n",
    "        if 'slidingWindowCoverage' not in kwargs:\n",
    "            self.slidingWindowCoverage  = 5\n",
    "            self.logger.warningMsg( f\"The size of the transformation window has not been specified. By default, it has been set on {self.slidingWindowCoverage}.\" )\n",
    "        else: self.slidingWindowCoverage= kwargs['slidingWindowCoverage']\n",
    "        if (loadFromPath is None) & (model is None):\n",
    "            self.logger.errorMsg( \"Cannot test a model if no model is indicated. Either specify the torch model object or a path to a .pth file containing a model.\" )\n",
    "            sys.exit( 1 )\n",
    "        if loadFromPath is not None:\n",
    "            if modelType is None:\n",
    "                self.logger.errorMsg( \"The type of model must be specified. Options: 'lstm', 'transformer'.\" )\n",
    "                sys.exit( 1 )\n",
    "            thisModel,optimizer = self._createModel( modelType=modelType,**kwargs )\n",
    "            thisModel,optimizer    = self._loadCheckPoint( loadFromPath, thisModel, optimizer )\n",
    "            self.logger.infoMsg( \"Checkpoint loaded successfully. Resuming training...\" )\n",
    "        if pathScaler is None:\n",
    "            self.logger.errorMsg( \"No scaler has been specified. This is required in order to test the model. Make sure you provide the path to the scaler.\" )\n",
    "            sys.exit( 1 )\n",
    "        self.scaler             = joblib.load( pathScaler )\n",
    "        self.logger.infoMsg( \"The scaler has been loaded successfully.\" )\n",
    "        thisModel.eval(  )\n",
    "        nSamples                = 0\n",
    "        nCorrectSamples         = 0\n",
    "        self.dataSets           = [ ]\n",
    "        self.blockWeights       = [ ]\n",
    "        allYPred                = [  ]\n",
    "        allYTrue                = [  ]\n",
    "        self.loadData( pathTest,blockWeights )\n",
    "        X_test, y_test        = self.extractFeatures( self.dataSets[ 0 ],self.blockWeights[ 0 ],modelType=modelType,slidingWindow=self.slidingWindow,fitScaler=fitScaler )\n",
    "        if len( self.dataSets )>1:\n",
    "            for k,ds in enumerate( self.dataSets[1:] ):\n",
    "                self.logger.infoMsg( f\"Extracting features for dataset No. {k+2}.\" )\n",
    "                X_test_temp, y_test_temp    = self.extractFeatures( ds,self.blockWeights[ k+1 ],modelType=modelType,slidingWindow=self.slidingWindow,fitScaler=fitScaler )\n",
    "                X_test                      = np.concatenate( [X_test,X_test_temp],axis=0 )\n",
    "                y_test                      = np.concatenate( [y_test,y_test_temp] )\n",
    "        self.logger.infoMsg( f\"The testing dataset has the following shape: {X_test.shape[0]} X {X_test.shape[1]}. The response: {y_test.shape}.\" )\n",
    "        X_test                  = self.scaler.transform( X_test )\n",
    "        X_test, y_test          = self.wrapTensor( X_test, y_test,slidingWindow=self.slidingWindowCoverage )\n",
    "        X_test                  = torch.from_numpy( X_test.astype(np.float32) )\n",
    "        y_test                  = torch.from_numpy( y_test.astype(np.float32) )\n",
    "        y_test                  = torch.argmax( y_test, dim=1 )\n",
    "        for i in range( X_test.shape[0]//batchSize ):\n",
    "            small_X_test        = X_test[  i*batchSize:i*batchSize+batchSize  ]\n",
    "            small_X_test        = small_X_test.to( device )\n",
    "            with torch.no_grad(  ):\n",
    "                yPred               = thisModel( small_X_test )\n",
    "                small_y_test        = y_test[  i*batchSize:i*batchSize+batchSize  ]\n",
    "                small_y_test        = small_y_test.to( device )\n",
    "                _, yPredLabels      = torch.max( yPred, dim=1 )\n",
    "                yPred_np            = yPredLabels.cpu(  ).numpy(  )\n",
    "                yTrue_np            = small_y_test.cpu(  ).numpy(  )\n",
    "                if len( yTrue_np.shape ) > 1 and yTrue_np.shape[1] > 1:\n",
    "                    yTrue_np        = np.argmax( yTrue_np, axis=1 )\n",
    "                allYPred.extend( yPred_np )\n",
    "                allYTrue.extend( yTrue_np )\n",
    "                nCorrectSamples     += (yPred_np == yTrue_np).sum(  )\n",
    "                nSamples            += batchSize\n",
    "        accuracy = nCorrectSamples / nSamples\n",
    "        self.logger.infoMsg(f'Accuracy: {accuracy:.4f}')\n",
    "        confMatrix = confusion_matrix(  allYTrue, allYPred, labels=np.arange( 11 )  )\n",
    "        classNames = [  self.nom.DICT_RIG_STATES[ i ] for i in self.nom.GOAL_RIG_STATES  ]\n",
    "        self._plotFancyContingencyTable( confMatrix, classNames )\n",
    "        self.printOutF1Scores( confMatrix,None,classNames )\n",
    "        return np.array( allYPred )\n",
    "    \n",
    "    def printOutF1Scores( self,confMatrix,nClasses=None,classesNames=None ):\n",
    "        if nClasses is None:\n",
    "            nClasses                = confMatrix.shape[0]\n",
    "            self.logger.warningMsg( f\"The number of classes has been inferred from the confusion matrix: {confMatrix.shape[0]}\" )\n",
    "        precScores      = [  ]\n",
    "        recallScores    = [  ]\n",
    "        f1Scores        = [  ]\n",
    "        for i in range( nClasses ):\n",
    "            TP = confMatrix[i, i]\n",
    "            FP = confMatrix[:, i].sum( ) - TP\n",
    "            FN = confMatrix[i, :].sum( ) - TP\n",
    "            precision = TP / (TP + FP) if TP + FP > 0 else 0\n",
    "            recall = TP / (TP + FN) if TP + FN > 0 else 0\n",
    "            f1 = 2 * ( precision * recall ) / ( precision + recall ) if precision + recall > 0 else 0\n",
    "            precScores.append(precision)\n",
    "            recallScores.append(recall)\n",
    "            f1Scores.append(f1)\n",
    "        if classesNames is None:        classesNames = [ f\"{i}\" for i in range(len(f1Scores)) ]\n",
    "        for i, (f1, precision, recall) in enumerate(zip( f1Scores, precScores, recallScores )):\n",
    "            self.logger.resultMessage( f\"Class {classesNames[i]}: Precision={precision:.2f}, Recall={recall:.2f}, F1 Score={f1:.2f}\" )\n",
    "        return precScores,recallScores,f1Scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bCA-lw81OjAY",
    "outputId": "8e259dc6-ad1e-4bf6-97b7-fd6b1615e711"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING]......... The block weight for the labeled_training_5632_11272024_2142 well was not provided. Therefore, it has been inferred. Its value is: 37.4\n",
      "[INFO]......... Block weight for labeled_training_5632_11272024_2142.csv has been set on: 37.4 klb.\n",
      "[INFO]......... Data has been loaded correctly to the trainer. In total, 1 dataframes have been loaded.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(  )\n",
    "trainer.loadData( r\"C:\\Users\\abrah\\OneDrive\\Doctorate Petroleum Engineering\\0A. RESEARCH PROJECT\\16. RIG ACTIVITY ENGINE\\0. DATA\\2. ANNOTATED TRAINING DATA\" )#,blockWeights={\"ANNOTATED_mb2314\":158} )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "PDJsxXbPOq4N",
    "outputId": "8cb7185c-0e45-4f99-a6d3-8ed3c58f9c7b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abrah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[INFO]......... The transformer model has been created.\n",
      "[WARNING]......... The size of the sliding windows (two resolution levels) has not been specified. By default, the small time window has been set on 5 sec, and the large one to 30 sec.\n",
      "[WARNING]......... The size of the transformation window has not been specified. By default, it has been set on 5.\n",
      "[INFO]......... Starting training...\n",
      "[INFO]......... Working on: GPU, model NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "[INFO]......... Extracting features for dataset No. 1.\n",
      "[INFO]......... The training dataset has the following shape: 836078 X 10. The response: (836078, 11)\n",
      "[INFO]......... The number of unique classes are: 11\n",
      "[INFO]......... Before undersampling, the counts per class are: {0: 13844, 1: 24500, 2: 3430, 3: 1588, 4: 20995, 5: 2758, 6: 127791, 7: 468570, 8: 85377, 9: 59631, 10: 27589}.\n",
      "[INFO]......... After undersampling, the counts per class are: {0: 13844, 1: 24500, 2: 3430, 3: 1588, 4: 20995, 5: 2758, 6: 85377, 7: 85377, 8: 85377, 9: 59631, 10: 27589}.\n",
      "[INFO]......... Size of the training data: 0.08 GB for the input matrix and 0.00 GB for the response variable.\n",
      "[INFO]......... Successfully saved scaler: scaler_transformer.pkl\n",
      "[INFO]......... Data loaded to the torch DataLoader object.\n",
      "[INFO]......... Initiating forward pass (epoch 0).\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 1/50, Train Loss: 1.6542\n",
      "[INFO]......... Checkpoint saved at epoch 0.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 2/50, Train Loss: 1.5946\n",
      "[INFO]......... Checkpoint saved at epoch 1.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 3/50, Train Loss: 1.5927\n",
      "[INFO]......... Checkpoint saved at epoch 2.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 4/50, Train Loss: 1.5894\n",
      "[INFO]......... Checkpoint saved at epoch 3.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 5/50, Train Loss: 1.5812\n",
      "[INFO]......... Checkpoint saved at epoch 4.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 6/50, Train Loss: 1.5888\n",
      "[INFO]......... Checkpoint saved at epoch 5.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 7/50, Train Loss: 1.5779\n",
      "[INFO]......... Checkpoint saved at epoch 6.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 8/50, Train Loss: 1.5700\n",
      "[INFO]......... Checkpoint saved at epoch 7.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 9/50, Train Loss: 1.5757\n",
      "[INFO]......... Checkpoint saved at epoch 8.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 10/50, Train Loss: 1.5787\n",
      "[INFO]......... Checkpoint saved at epoch 9.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 11/50, Train Loss: 1.5674\n",
      "[INFO]......... Checkpoint saved at epoch 10.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 12/50, Train Loss: 1.5759\n",
      "[INFO]......... Checkpoint saved at epoch 11.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 13/50, Train Loss: 1.5681\n",
      "[INFO]......... Checkpoint saved at epoch 12.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 14/50, Train Loss: 1.5684\n",
      "[INFO]......... Checkpoint saved at epoch 13.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 15/50, Train Loss: 1.5631\n",
      "[INFO]......... Checkpoint saved at epoch 14.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 16/50, Train Loss: 1.5752\n",
      "[INFO]......... Checkpoint saved at epoch 15.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 17/50, Train Loss: 1.5744\n",
      "[INFO]......... Checkpoint saved at epoch 16.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 18/50, Train Loss: 1.5664\n",
      "[INFO]......... Checkpoint saved at epoch 17.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 19/50, Train Loss: 1.5602\n",
      "[INFO]......... Checkpoint saved at epoch 18.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 20/50, Train Loss: 1.5693\n",
      "[INFO]......... Checkpoint saved at epoch 19.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 21/50, Train Loss: 1.5690\n",
      "[INFO]......... Checkpoint saved at epoch 20.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 22/50, Train Loss: 1.5672\n",
      "[INFO]......... Checkpoint saved at epoch 21.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 23/50, Train Loss: 1.5657\n",
      "[INFO]......... Checkpoint saved at epoch 22.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 24/50, Train Loss: 1.5761\n",
      "[INFO]......... Checkpoint saved at epoch 23.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 25/50, Train Loss: 1.5635\n",
      "[INFO]......... Checkpoint saved at epoch 24.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 26/50, Train Loss: 1.5685\n",
      "[INFO]......... Checkpoint saved at epoch 25.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 27/50, Train Loss: 1.5629\n",
      "[INFO]......... Checkpoint saved at epoch 26.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 28/50, Train Loss: 1.5595\n",
      "[INFO]......... Checkpoint saved at epoch 27.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 29/50, Train Loss: 1.5650\n",
      "[INFO]......... Checkpoint saved at epoch 28.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 30/50, Train Loss: 1.5646\n",
      "[INFO]......... Checkpoint saved at epoch 29.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 31/50, Train Loss: 1.5649\n",
      "[INFO]......... Checkpoint saved at epoch 30.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 32/50, Train Loss: 1.5710\n",
      "[INFO]......... Checkpoint saved at epoch 31.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 33/50, Train Loss: 1.5692\n",
      "[INFO]......... Checkpoint saved at epoch 32.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 34/50, Train Loss: 1.5614\n",
      "[INFO]......... Checkpoint saved at epoch 33.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 35/50, Train Loss: 1.5570\n",
      "[INFO]......... Checkpoint saved at epoch 34.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 36/50, Train Loss: 1.5611\n",
      "[INFO]......... Checkpoint saved at epoch 35.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 37/50, Train Loss: 1.5639\n",
      "[INFO]......... Checkpoint saved at epoch 36.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 38/50, Train Loss: 1.5591\n",
      "[INFO]......... Checkpoint saved at epoch 37.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 39/50, Train Loss: 1.5593\n",
      "[INFO]......... Checkpoint saved at epoch 38.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 40/50, Train Loss: 1.5584\n",
      "[INFO]......... Checkpoint saved at epoch 39.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 41/50, Train Loss: 1.5635\n",
      "[INFO]......... Checkpoint saved at epoch 40.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 42/50, Train Loss: 1.5647\n",
      "[INFO]......... Checkpoint saved at epoch 41.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 43/50, Train Loss: 1.5660\n",
      "[INFO]......... Checkpoint saved at epoch 42.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 44/50, Train Loss: 1.5563\n",
      "[INFO]......... Checkpoint saved at epoch 43.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 45/50, Train Loss: 1.5612\n",
      "[INFO]......... Checkpoint saved at epoch 44.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 46/50, Train Loss: 1.5654\n",
      "[INFO]......... Checkpoint saved at epoch 45.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 47/50, Train Loss: 1.5594\n",
      "[INFO]......... Checkpoint saved at epoch 46.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 48/50, Train Loss: 1.5534\n",
      "[INFO]......... Checkpoint saved at epoch 47.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 49/50, Train Loss: 1.5527\n",
      "[INFO]......... Checkpoint saved at epoch 48.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 0.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 150.\n",
      "[INFO]......... [LOADER>>>]..... Processing batch No. 300.\n",
      "[INFO]......... [TRAINING MSG>>>]..... Epoch 50/50, Train Loss: 1.5540\n",
      "[INFO]......... Checkpoint saved at epoch 49.\n",
      "[INFO]......... Successfully saved checkpoint: transformer.chpt\n",
      "[INFO]......... Successfully saved transformer model: model_transformer.pth\n"
     ]
    }
   ],
   "source": [
    "trainLosses, model=trainer.trainModel( \"transformer\",batchSize=1024,nEpochs=50 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "l-0metCYZWmu",
    "outputId": "1f891e0c-429b-4a5a-e9b3-319fab7783ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abrah\\AppData\\Local\\Temp\\ipykernel_25968\\2997390695.py:253: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels( ax.get_xticklabels( ), fontdict=self.FDICT_PLOTS )\n",
      "C:\\Users\\abrah\\AppData\\Local\\Temp\\ipykernel_25968\\2997390695.py:254: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels( ax.get_yticklabels( ), fontdict=self.FDICT_PLOTS )\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 683x333 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.plotLosses( trainLosses,None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]......... Testing on: GPU, model NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "[WARNING]......... The size of the sliding windows (two resolution levels) has not been specified. By default, the small time window has been set on 5 sec, and the large one to 30 sec.\n",
      "[WARNING]......... The size of the transformation window has not been specified. By default, it has been set on 5.\n",
      "c:\\Users\\abrah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[INFO]......... Checkpoint loaded successfully. Resuming training...\n",
      "[INFO]......... The scaler has been loaded successfully.\n",
      "[WARNING]......... The block weight for the labeled_testing_5632_11272024_2142 well was not provided. Therefore, it has been inferred. Its value is: 48.4\n",
      "[INFO]......... Block weight for labeled_testing_5632_11272024_2142.csv has been set on: 48.4 klb.\n",
      "[INFO]......... Data has been loaded correctly to the trainer. In total, 1 dataframes have been loaded.\n"
     ]
    }
   ],
   "source": [
    "trainer=Trainer(  )\n",
    "preds = trainer.testModel( r\"C:\\Users\\abrah\\OneDrive\\Doctorate Petroleum Engineering\\0A. RESEARCH PROJECT\\16. RIG ACTIVITY ENGINE\\0. DATA\\temp3\",\n",
    "                  pathScaler=r\"C:\\Users\\abrah\\OneDrive\\Doctorate Petroleum Engineering\\0A. RESEARCH PROJECT\\16. RIG ACTIVITY ENGINE\\3. CHECKPOINTS\\Stratified Models\\TRANSFORMER_TEMP\\scaler_transformer.pkl\",\n",
    "                  modelType='transformer',\n",
    "                  loadFromPath=r\"C:\\Users\\abrah\\OneDrive\\Doctorate Petroleum Engineering\\0A. RESEARCH PROJECT\\16. RIG ACTIVITY ENGINE\\3. CHECKPOINTS\\Stratified Models\\TRANSFORMER_TEMP\\transformer_transformer_checkpoint.chpt_chk\",\n",
    "                  # slidingWindow=5\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on unseen datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bTNBbonFVOef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO]......... Testing on: GPU, model NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "c:\\Users\\abrah\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "[INFO]......... Checkpoint loaded successfully. Resuming training...\n",
      "[ERROR]......... No scaler has been specified. This is required in order to test the model. Make sure you provide the path to the scaler.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abrah\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(  )\n",
    "trainer.testModel( pathTest=r\"C:\\Users\\abrah\\OneDrive\\Doctorate Petroleum Engineering\\0A. RESEARCH PROJECT\\16. RIG ACTIVITY ENGINE\\0. DATA\\temp\",\n",
    "                  blockWeights={ \"ANNOTATED_va9\":190.0 },\n",
    "                  batchSize=1024,loadFromPath=r\"C:\\Users\\abrah\\OneDrive\\Doctorate Petroleum Engineering\\0A. RESEARCH PROJECT\\16. RIG ACTIVITY ENGINE\\3. CHECKPOINTS\\20241103_0600_UF16A7832_checkpoint\",\n",
    "                  modelType='transformer',model=None,slidingWindow=15 )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
